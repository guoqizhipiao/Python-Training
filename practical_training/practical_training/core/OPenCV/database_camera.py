import sys
import tkinter as tk
from tkinter import messagebox
import cv2
from PIL import Image, ImageTk,ImageDraw,ImageFont
import threading
import os
import json
import numpy as np
from collections import deque
import time
# ======================
# é…ç½®åŒºï¼ˆæ ¹æ®ä½ çš„ç¯å¢ƒä¿®æ”¹ï¼‰
# ======================

opencv_path = os.path.dirname(os.path.abspath(__file__))
core_path = os.path.dirname(opencv_path)
practical_training_path = os.path.dirname(core_path)
sys.path.append(practical_training_path)

from core.facerecognition.facedatamatch import face_data_match_from_frame

# åŠ è½½äººè„¸è¯†åˆ«æ¨¡å‹

cascade_path = cv2.data.haarcascades + 'haarcascade_frontalface_alt2.xml'
face_detector = cv2.CascadeClassifier(cascade_path)


class FaceRecognitionApp:
    def __init__(self, root, recognizer, NAMES):

        self.frame_deque = deque()
        self.frame_deque_lock = threading.Lock()  # æ·»åŠ é”
        self.face_data_match_deque = deque()
        self.face_data_match_lock = threading.Lock()  # æ·»åŠ é”

        self.recognizer = recognizer
        self.NAMES = NAMES
        self.root = root
        self.root.title("äººè„¸è¯†åˆ«ç³»ç»Ÿ")
        # è·å–å±å¹•å®½åº¦å’Œé«˜åº¦
        self.screen_width = self.root.winfo_screenwidth()
        self.screen_height = self.root.winfo_screenheight()
        # è®¾å®šçª—å£å¤§å°
        self.window_width = int(self.screen_width * 0.5)
        self.window_height = int(self.screen_height * 0.5)
        # è®¡ç®—çª—å£å·¦ä¸Šè§’åæ ‡ï¼Œä½¿å…¶å±…ä¸­
        self.x = (self.screen_width - self.window_width) // 2
        self.y = (self.screen_height - self.window_height) // 2
        # è®¾ç½®çª—å£æœ€å°å®½åº¦ä¸º 400ï¼Œæœ€å°é«˜åº¦ä¸º 300
        self.root.minsize(400, 300)
        #è®¾ç½®å¤§å°å’Œä½ç½®
        self.root.geometry(f"{self.window_width}x{self.window_height}+{self.x}+{self.y}")
        self.root.resizable(True, True)

        # åˆ›å»ºèœå•
        self.menu = TopMenu(self)
        self.root.config(menu=self.menu)

        # è§†é¢‘æ˜¾ç¤ºåŒºåŸŸ
        self.video_label = tk.Label(root, bg='black')
        self.video_label.pack(expand=True, fill=tk.BOTH)

        # æ§åˆ¶å˜é‡
        self.is_running = False
        self.cap = None

        #self.start_recognition()

        self.root.protocol("WM_DELETE_WINDOW", self.on_closing)

    def on_closing(self):
        self.is_running = False
        if self.cap:
            self.cap.release()
        self.video_label.config(image='')
        self.root.destroy()


    def start_recognition(self):
        print(self.recognizer)
        for i in self.NAMES:
            print(i,self.NAMES[i])
        if not self.is_running:
            self.is_running = True
            self.cap = cv2.VideoCapture(0)
            if not self.cap.isOpened():
                messagebox.showerror("é”™è¯¯", "æ— æ³•æ‰“å¼€æ‘„åƒå¤´ï¼")
                self.is_running = False
                return
            self.update_frame()

    def stop_recognition(self):
        self.is_running = False
        if self.cap:
            self.cap.release()
        self.video_label.config(image='')  # æ¸…ç©ºç”»é¢



    def update_frame(self):
        print("å¯åŠ¨å¤šçº¿ç¨‹äººè„¸è¯†åˆ«")

        self.match_confidence = 0
        self.cv2_confidence = 0
        self.cv2_text = ""
        self.match_text = ""

        self.t1 = threading.Thread(target=self.face_data_match_from_frame_thread, daemon=True)
        self.t1.start()
        self.update_frame_main()


    def update_frame_main(self):
        if not self.is_running:
            return

        # æ¸…ç©ºé˜Ÿåˆ—ï¼Œä¿æŒæœ€æ–°ä¸€å¸§
        while len(self.frame_deque) > 1:
            self.frame_deque.pop()

        ret, frame = self.cap.read()
        if ret: 
            # å°†å½“å‰å¸§æ”¾å…¥é˜Ÿåˆ—
            self.frame_deque.append(frame)
            cv2_return = self.cv2_face_recognize(frame)

            if cv2_return:
                w1, w2 = 0.7, 0.3
                frame, self.cv2_text, self.cv2_confidence, x, y = cv2_return
                #print("OpenCVè¯†åˆ«ç»“æœ:", self.cv2_text, self.cv2_confidence)
                self.cv2_confidence = 1 - (self.cv2_confidence/80)
                self.cv2_confidence = w2 * self.cv2_confidence
                text =  self.cv2_text
                if len(self.face_data_match_deque) > 0:

                    with self.face_data_match_lock:
                        match_result = self.face_data_match_deque.pop()

                    if isinstance(match_result, tuple) and match_result[0] == "SUCCESS":
                        self.match_text = match_result[2]
                        self.match_confidence = match_result[5]

                        self.match_confidence = 1 - (self.match_confidence / 0.4)
                        self.match_confidence = w1 * self.match_confidence

                    elif match_result == "NOMATCH":
                        self.match_text = "æœªçŸ¥äººå‘˜"

                print("==========opencvè¯†åˆ«ç»“æœ:", self.cv2_text, self.cv2_confidence)
                print("==========mathè¯†åˆ«ç»“æœ:", self.match_text, self.match_confidence)

                if self.match_text == self.cv2_text:
                    text = self.match_text
                elif self.match_text != self.cv2_text:
                    if self.match_confidence > self.cv2_confidence:
                        text = self.match_text
                    else:
                        text = self.cv2_text
                elif self.match_text == "æœªçŸ¥äººå‘˜":
                    text = self.match_text

                frame = self.cv2_putText_chinese(
                frame,
                text,
                pos=(x + 10, y - 30),  # æ³¨æ„ï¼šPIL çš„ y åæ ‡å¯èƒ½éœ€è¦å¾®è°ƒï¼ˆæ¯” OpenCV é«˜ä¸€ç‚¹ï¼‰
                font_path="simsun.ttc",  # Windows ç³»ç»Ÿå®‹ä½“
                font_size=24,
                color=(0, 255, 0))

            # è½¬ä¸º PIL å›¾åƒå¹¶åœ¨ Tkinter ä¸­æ˜¾ç¤º
            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            img = Image.fromarray(rgb_frame)
            imgtk = ImageTk.PhotoImage(image=img)
            self.video_label.imgtk = imgtk
            self.video_label.configure(image=imgtk)

        # æ¯ 20ms æ›´æ–°ä¸€å¸§
        self.video_label.after(20, self.update_frame_main)


    def cv2_face_recognize(self, frame):

            # äººè„¸è¯†åˆ«å¤„ç†
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            faces = face_detector.detectMultiScale(
                gray, scaleFactor=1.2, minNeighbors=5,
                minSize=(60, 60), maxSize=(1000, 1000)
            )
            for (x, y, w, h) in faces:
                # ç”»åœ†å½¢æ¡†
                center = (x + w // 2, y + h // 2)
                radius = w // 2
                cv2.circle(frame, center, radius, (0, 255, 0), 2)

                # é¢„æµ‹
                try:
                    ids, confidence = self.recognizer.predict(gray[y:y + h, x:x + w])

                    if confidence > 80:
                        confidence = 80
                        text = "æœªçŸ¥äººå‘˜"
                    else:
                        name = self.NAMES.get(str(ids), "æœªçŸ¥äººå‘˜")
                        text = name

                    return (frame, text, confidence, x, y)
                    break

                except Exception as e:
                    print("é¢„æµ‹å‡ºé”™:", e)

    def face_data_match_from_frame_thread(self):
        if not self.is_running:
            return
        while self.is_running:
            if len(self.frame_deque) == 0:
                time.sleep(0.05)
                continue
            # è·å–æœ€æ–°ä¸€å¸§
            with self.frame_deque_lock:
                frame = self.frame_deque[-1].copy()
            # è°ƒç”¨åŒ¹é…å‡½æ•°
            result = face_data_match_from_frame(frame)
            if result == "NOFACE":
                print("æœªæ£€æµ‹åˆ°äººè„¸")
            elif isinstance(result, tuple) and result[0] == "SUCCESS":
                print("è¯†åˆ«ç»“æœ:", result)
                time.sleep(1)  # æ§åˆ¶å¤„ç†é¢‘ç‡
            elif result == "NOMATCH":
                print("æ²¡æœ‰åŒ¹é…åˆ°å·²æ³¨å†Œçš„å­¦ç”Ÿ")
                time.sleep(1)  # æ§åˆ¶å¤„ç†é¢‘ç‡

            with self.face_data_match_lock:
                self.face_data_match_deque.append(result)


    def cv2_putText_chinese(self, img, text, pos, font_path="simsun.ttc", font_size=30, color=(0, 255, 0)):
        """
        åœ¨ OpenCV å›¾åƒä¸Šç»˜åˆ¶ä¸­æ–‡
    
        å‚æ•°:
            img: OpenCV å›¾åƒ (BGR æ ¼å¼)
            text: è¦ç»˜åˆ¶çš„ä¸­æ–‡æ–‡æœ¬
            pos: æ–‡æœ¬å·¦ä¸Šè§’åæ ‡ (x, y)
            font_path: ä¸­æ–‡å­—ä½“æ–‡ä»¶è·¯å¾„ï¼ˆWindows å¸¸ç”¨ simsun.ttc æˆ– msyh.ttcï¼‰
            font_size: å­—ä½“å¤§å°
            color: BGR é¢œè‰² (æ³¨æ„ï¼šOpenCV æ˜¯ BGRï¼ŒPIL æ˜¯ RGBï¼Œè¿™é‡Œåšè½¬æ¢)
        """
        # å°† OpenCV çš„ BGR å›¾åƒè½¬ä¸º RGBï¼ˆä¾› PIL ä½¿ç”¨ï¼‰
        img_pil = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
        draw = ImageDraw.Draw(img_pil)
    
        try:
            font = ImageFont.truetype(font_path, font_size)
        except IOError:
            # å¦‚æœæŒ‡å®šå­—ä½“æ‰¾ä¸åˆ°ï¼Œä½¿ç”¨é»˜è®¤å­—ä½“ï¼ˆå¯èƒ½ä»ä¸æ”¯æŒä¸­æ–‡ï¼‰
            font = ImageFont.load_default()
            print("è­¦å‘Šï¼šæœªæ‰¾åˆ°ä¸­æ–‡å­—ä½“ï¼Œä¸­æ–‡å¯èƒ½æ˜¾ç¤ºä¸ºæ–¹æ¡†ã€‚")
    
        # PIL ä½¿ç”¨ RGBï¼Œæ‰€ä»¥è¦æŠŠ BGR çš„ color è½¬æˆ RGB
        rgb_color = (color[2], color[1], color[0])  # BGR â†’ RGB
        draw.text(pos, text, fill=rgb_color, font=font)
    
        # è½¬å› OpenCV çš„ BGR æ ¼å¼
        img_cv = cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR)
        return img_cv

class TopMenu(tk.Menu):
    def __init__(self, app: FaceRecognitionApp):
        super().__init__()
        self.app = app
        self.file_menu()
        self.operation_menu()
        self.help_menu()

    def file_menu(self):
        file_menu = tk.Menu(self, tearoff=False)
        file_menu.add_command(label='é€€å‡º', command=self.quit_app)
        self.add_cascade(label='æ–‡ä»¶', menu=file_menu)

    def operation_menu(self):
        op_menu = tk.Menu(self, tearoff=False)
        op_menu.add_command(label='å¯åŠ¨è¯†åˆ«', command=self.app.start_recognition)
        op_menu.add_command(label='åœæ­¢è¯†åˆ«', command=self.app.stop_recognition)
        self.add_cascade(label='æ“ä½œ', menu=op_menu)

    def help_menu(self):
        help_menu = tk.Menu(self, tearoff=False)
        help_menu.add_command(label='å…³äº', command=self.show_about)
        self.add_cascade(label='å¸®åŠ©', menu=help_menu)

    def show_about(self):
        messagebox.showinfo("å…³äº", "äººè„¸è¯†åˆ«ç³»ç»Ÿ\nåŸºäº OpenCV + LBPH\nä½œè€…ï¼šä½ è‡ªå·± ğŸ˜Š")

    def quit_app(self):
        self.app.stop_recognition()
        self.app.root.destroy()  # é”€æ¯çª—å£



def camera(oprnvvgui,current_model):
    try:
        trainer_path = os.path.join(opencv_path, 'trainer', current_model)
        print("åŠ è½½æ¨¡å‹è·¯å¾„:", trainer_path)
        time_name = os.path.splitext(trainer_path)[0]
        json_path = f"{time_name}.json"
        with open(json_path, 'r', encoding='utf-8') as f:
            NAMES = json.load(f)
        for i in NAMES:
            print(f"åŠ è½½å§“å: ID={i}, å§“å={NAMES[i]}") 

        try:
            recognizer = cv2.face.LBPHFaceRecognizer_create()
            recognizer.read(trainer_path)
        except Exception as e:
            messagebox.showerror("é”™è¯¯", f"æ— æ³•åŠ è½½è®­ç»ƒæ¨¡å‹:\n{e}")

        root = tk.Toplevel(oprnvvgui)
        app = FaceRecognitionApp(root,recognizer,NAMES)
        root.mainloop()
    except Exception as e:
        print("å¯åŠ¨æ‘„åƒå¤´å¤±è´¥:", e)



# ======================
# ä¸»ç¨‹åºå…¥å£
# ======================
if __name__ == '__main__':
    root = tk.Tk()
    app = FaceRecognitionApp(root)
    root.protocol("WM_DELETE_WINDOW", root.quit_app)  # ç‚¹Ã—å…³é—­æ—¶é‡Šæ”¾èµ„æº
    root.mainloop()