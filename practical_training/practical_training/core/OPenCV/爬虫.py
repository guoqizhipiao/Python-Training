import os
import requests
from bs4 import BeautifulSoup
import re
import urllib.request
import sqlite3

ImageCount = 0


def GetPageURL(URLStr):
    # è·å–ä¸€ä¸ªé¡µé¢çš„æ‰€æœ‰å›¾ç‰‡çš„URL+ä¸‹é¡µçš„URL
    if not URLStr:
        print('ç°åœ¨æ˜¯æœ€åä¸€é¡µå•¦ï¼çˆ¬å–ç»“æŸ')
        return [], ''
    try:
        header = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/27.0.1453.93 Safari/537.36"
        }
        response = requests.get(URLStr, headers=header)
        response.encoding = 'utf-8'
        html = response.text
    except Exception as e:
        print("err=", str(e))
        return [], ''

    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–å›¾ç‰‡URL
    ImageURL = re.findall(r'"objURL":"(.*?)",', html, re.S)

    # ä½¿ç”¨BeautifulSoupè§£æHTMLï¼Œæå–ä¸‹ä¸€é¡µçš„URL
    soup = BeautifulSoup(html, 'html.parser')
    NextPageURLS = soup.find('a', class_='n', text='ä¸‹ä¸€é¡µ')
    if NextPageURLS:
        NextPageURL = 'http://image.baidu.com' + NextPageURLS['href']
    else:
        NextPageURL = ''

    return ImageURL, NextPageURL


def DownLoadImage(pic_urls, ImageFilePath, keyword, db_conn, max_images):
    """ç»™å‡ºå›¾ç‰‡é“¾æ¥åˆ—è¡¨, ä¸‹è½½æ‰€æœ‰å›¾ç‰‡"""
    global ImageCount
    for pic_url in pic_urls:
        # å¦‚æœå·²ç»è¾¾åˆ°æœ€å¤§ä¸‹è½½æ•°é‡ï¼Œé€€å‡ºå¾ªç¯
        if ImageCount >= max_images:
            print(f"å·²è¾¾åˆ°æœ€å¤§ä¸‹è½½æ•°é‡ {max_images}ï¼Œåœæ­¢ä¸‹è½½")
            break

        try:
            cursor = db_conn.cursor()
            cursor.execute("SELECT 1 FROM images WHERE original_url = ?", (pic_url,))
            if cursor.fetchone():
                print(f"è·³è¿‡å·²å­˜åœ¨çš„å›¾ç‰‡: {pic_url}")
                continue

            pic = requests.get(pic_url, timeout=15)
            ImageCount += 1
            local_filename = f"{ImageCount}.jpg"
            local_path = os.path.join(ImageFilePath, local_filename)

            with open(local_path, 'wb') as f:
                f.write(pic.content)

            # æ’å…¥æ•°æ®åº“
            cursor.execute('''
                INSERT OR IGNORE INTO images 
                (keyword, original_url, local_path, source_page)
                VALUES (?, ?, ?, ?)
            ''', (keyword, pic_url, local_path, "https://image.baidu.com"))

            db_conn.commit()
            print(f'âœ… å·²ä¸‹è½½ç¬¬{ImageCount}å¼ å›¾ç‰‡: {pic_url}')

        except Exception as e:
            print(f'âŒ ä¸‹è½½å¤±è´¥: {pic_url}, é”™è¯¯: {e}')
            continue


def CreateDirectory(path):
    """åˆ›å»ºç›®å½•ï¼Œå¦‚æœä¸å­˜åœ¨çš„è¯"""
    if not os.path.exists(path):
        os.makedirs(path)


def CrawlPicture(keyword, max_images):
    # è·å–ç”¨æˆ·çš„æ¡Œé¢è·¯å¾„ï¼Œå¹¶åˆ›å»ºä¿å­˜å›¾ç‰‡çš„ç›®å½•
    desktop_path = os.path.join(os.path.expanduser("~"), 'Desktop')
    picture_path = os.path.join(desktop_path, 'picture')
    CreateDirectory(picture_path)

    # åˆ›å»ºä»¥å…³é”®å­—å‘½åçš„å­æ–‡ä»¶å¤¹
    keyword_path = os.path.join(picture_path, keyword)
    CreateDirectory(keyword_path)

    # === æ–°å¢ï¼šåˆå§‹åŒ–æ•°æ®åº“ ===
    db_path = os.path.join(picture_path, 'image_metadata.db')
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS images (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            keyword TEXT NOT NULL,
            original_url TEXT UNIQUE NOT NULL,   -- é˜²æ­¢é‡å¤çˆ¬å–
            local_path TEXT NOT NULL,
            source_page TEXT,
            downloaded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    ''')
    conn.commit()

    # åˆå§‹åŒ–çˆ¬å–æ ‡å¿—
    CrawlFlag = True
    NextPageURL = f"https://image.baidu.com/search/flip?tn=baiduimage&ps=1&ct=201326592&lm=-1&cl=2&nc=1&ie=utf-8&word={urllib.parse.quote(keyword, safe='/')}"

    while CrawlFlag:
        ImageURL, NextPageURL = GetPageURL(NextPageURL)
        if ImageURL:
            unique_urls = list(set(ImageURL))
            DownLoadImage(unique_urls, keyword_path, keyword, conn, max_images)
        if not NextPageURL or ImageCount >= max_images:
            CrawlFlag = False

    # çˆ¬å–ç»“æŸï¼Œå…³é—­æ•°æ®åº“
    conn.close()
    print(f"ğŸ‰ çˆ¬å–å®Œæˆï¼å…±ä¸‹è½½ {ImageCount} å¼ å›¾ç‰‡ï¼Œå…ƒæ•°æ®å·²ä¿å­˜è‡³ {db_path}")


if __name__ == '__main__':
    keyword = input("è¯·è¾“å…¥è¦çˆ¬å–çš„å…³é”®è¯: ")
    max_images = int(input("è¯·è¾“å…¥è¦ä¸‹è½½çš„æœ€å¤§å›¾ç‰‡æ•°é‡: "))
    CrawlPicture(keyword, max_images)